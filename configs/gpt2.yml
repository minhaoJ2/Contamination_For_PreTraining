dataset:
  is_split_by_sentences: true
  datasets:
    - "tomekkorbak/detoxify-pile-chunk3-0-50000"
    - "tomekkorbak/detoxify-pile-chunk3-50000-100000"
    - "tomekkorbak/detoxify-pile-chunk3-100000-150000"
    - "tomekkorbak/detoxify-pile-chunk3-150000-200000"
    - "tomekkorbak/detoxify-pile-chunk3-200000-250000"
    - "tomekkorbak/detoxify-pile-chunk3-250000-300000"
    - "tomekkorbak/detoxify-pile-chunk3-300000-350000"
    - "tomekkorbak/detoxify-pile-chunk3-350000-400000"
    - "tomekkorbak/detoxify-pile-chunk3-400000-450000"
    - "tomekkorbak/detoxify-pile-chunk3-450000-500000"
    - "tomekkorbak/detoxify-pile-chunk3-500000-550000"
    - "tomekkorbak/detoxify-pile-chunk3-550000-600000"
    - "tomekkorbak/detoxify-pile-chunk3-600000-650000"
    - "tomekkorbak/detoxify-pile-chunk3-650000-700000"
    - "tomekkorbak/detoxify-pile-chunk3-700000-750000"
    - "tomekkorbak/detoxify-pile-chunk3-750000-800000"
    - "tomekkorbak/detoxify-pile-chunk3-800000-850000"
    - "tomekkorbak/detoxify-pile-chunk3-850000-900000"
    - "tomekkorbak/detoxify-pile-chunk3-900000-950000"
    - "tomekkorbak/detoxify-pile-chunk3-950000-1000000"
    - "tomekkorbak/detoxify-pile-chunk3-1000000-1050000"
    - "tomekkorbak/detoxify-pile-chunk3-1050000-1100000"
    - "tomekkorbak/detoxify-pile-chunk3-1100000-1150000"
    - "tomekkorbak/detoxify-pile-chunk3-1150000-1200000"
    - "tomekkorbak/detoxify-pile-chunk3-1200000-1250000"
    - "tomekkorbak/detoxify-pile-chunk3-1250000-1300000"
    - "tomekkorbak/detoxify-pile-chunk3-1300000-1350000"
    - "tomekkorbak/detoxify-pile-chunk3-1350000-1400000"
    - "tomekkorbak/detoxify-pile-chunk3-1400000-1450000"
    - "tomekkorbak/detoxify-pile-chunk3-1450000-1500000"
    - "tomekkorbak/detoxify-pile-chunk3-1500000-1550000"
    - "tomekkorbak/detoxify-pile-chunk3-1550000-1600000"
    - "tomekkorbak/detoxify-pile-chunk3-1600000-1650000"
    - "tomekkorbak/detoxify-pile-chunk3-1650000-1700000"
    - "tomekkorbak/detoxify-pile-chunk3-1700000-1750000"
    - "tomekkorbak/detoxify-pile-chunk3-1750000-1800000"
    - "tomekkorbak/detoxify-pile-chunk3-1800000-1850000"
    - "tomekkorbak/detoxify-pile-chunk3-1850000-1900000"
    - "tomekkorbak/detoxify-pile-chunk3-1900000-1950000"


tokenizer:
  path_or_name: gpt2

model:
  path_or_name: gpt2
  gpt2_config_kwargs:
      reorder_and_upcast_attn: true
      scale_attn_by: true

objective:
  name: MLE

training:
  # output_dir: /shared/data2/minhaoj2/gpt-2-text-agnews
  # output_dir: /lfs/local/0/kzliu/contamination/contamination_analysis/outputs
  output_dir: ./outputs

  # NOTE: this is NOT an actual `Trainer` arg, but used in `prepare_trainer_arguments`
  effective_batch_size: 32

  num_tokens: 3.3E+9  # Comment out if want to train based on epochs
  num_train_epochs: 1  # kzl: not used yet

  learning_rate: 0.0005
  # kzl: 8 gpu training
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 2  # If GPU is OOM (smaller GPUs e.g. w/ 16GB)

  # # kzl: For 2 GPU training
  # per_device_train_batch_size: 2
  # gradient_accumulation_steps: 8  # If GPU is OOM (smaller GPUs e.g. w/ 16GB)

  # # kzl: For single GPU training
  # per_device_train_batch_size: 2
  # gradient_accumulation_steps: 16  # If GPU is OOM (smaller GPUs e.g. w/ 16GB)

  fp16: true
  weight_decay: 0.1
  evaluation_strategy: 'no'
  logging_steps: 1
  warmup_ratio: 0.01
  logging_first_step: true
  seed: 42
  remove_unused_columns: false
  dataloader_num_workers: 0  # kzl: no need to set with DDP
  save_strategy: steps
  save_steps: 50000