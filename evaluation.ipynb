{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset, load_metric\n",
    "from rouge import Rouge\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# config = GPT2Config()\n",
    "# model = GPT2LMHeadModel(config)\n",
    "# checkpoint = torch.load(\"/shared/data2/minhaoj2/gpt-2-original/pytorch_model.bin\")\n",
    "# model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"Preparing Model ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    input_text = f\"Question: To which category does the text belong?:” Positive Sentiment “, “ Negative Sentiment. Text: {text}. Answer: \"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_new_tokens=3)\n",
    "\n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output_text.split(\"Answer: \")[-1]\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
    "preds = []\n",
    "labels = []\n",
    "for data in tqdm(dataset):\n",
    "    ground_truth = data['label']\n",
    "    labels.append(ground_truth)\n",
    "    output = classify(data['sentence'])\n",
    "    if output == \"Positive Sentiment\":\n",
    "        preds.append(1)\n",
    "    elif output == \"Negative Sentiment\":\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(batch):\n",
    "    batch_input_text = [f\"Context: {context} Question: {question} Answer:\" \n",
    "                        for context, question in zip(batch['context'], batch['question'])]\n",
    "    input_ids = tokenizer(batch_input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)[\"input_ids\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_new_tokens=15, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    batch_answers = [tokenizer.decode(output, skip_special_tokens=True).replace(input_text, '').strip().split('.')[0]\n",
    "                      for input_text, output in zip(batch_input_text, outputs)]\n",
    "    batch['ans'] = batch_answers\n",
    "    return batch\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"validation\").select(range(100))\n",
    "ans_dataset = dataset.map(get_answer, batched=True, batch_size=4)\n",
    "ans = ans_dataset['ans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    "import re\n",
    "res = collections.defaultdict()\n",
    "for i in tqdm(range(len(ans))):\n",
    "    res[dataset[i]['id']] = ans[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"squad\", split=\"validation\")\n",
    "res = {}\n",
    "for data in tqdm(dataset):\n",
    "    id = data['id']\n",
    "    context = data['context']\n",
    "    question = data['question']\n",
    "    pred = data['ans']\n",
    "    res[id] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    "import re\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(lower(s))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "f1_scores = collections.defaultdict()\n",
    "for article in tqdm(dataset):\n",
    "    gold_answer = [a for a in article['answers']['text']\n",
    "                        if normalize_answer(a)]\n",
    "    qid = article['id']\n",
    "    a_pred = res[qid]\n",
    "    f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answer)\n",
    "total = len(f1_scores)\n",
    "print(f\"F1 Score: {100.0 * sum(f1_scores.values()) / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset, load_metric\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# config = GPT2Config()\n",
    "# model = GPT2LMHeadModel(config)\n",
    "# checkpoint = torch.load(\"/shared/data2/minhaoj2/gpt-2-original/pytorch_model.bin\")\n",
    "# model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "def classify(batch):\n",
    "    sentences = batch['sentence']\n",
    "    batch_input_text = [f\"Question: To which category does the text belong?: Positive Sentiment, Negative Sentiment. Text: {s}. Answer: \" for s in sentences]\n",
    "    input_ids = tokenizer(batch_input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)[\"input_ids\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_new_tokens=15, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    output_text = [tokenizer.decode(output, skip_special_tokens=True).split(\"Answer: \")[-1] for output in outputs]\n",
    "    batch['output'] = output_text\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train\").select(range(100))\n",
    "output_dataset = dataset.map(classify, batched=True, batch_size=16)\n",
    "outputs = output_dataset['output']\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(len(dataset)):\n",
    "    ground_truth = dataset[i]['label']\n",
    "    output = outputs[i]\n",
    "    if output == \"Positive Sentiment\":\n",
    "        preds.append(1)\n",
    "    elif output == \"Negative Sentiment\":\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        print(output)\n",
    "\n",
    "print(accuracy_score(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
