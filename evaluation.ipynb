{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset, load_metric\n",
    "from rouge import Rouge\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Model ...\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "# checkpoint = torch.load(\"./gpt-2-original/pytorch_model.bin\")\n",
    "# model.load_state_dict(checkpoint)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"Preparing Model ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sst2(model, tokenizer, device=device):\n",
    "    possible_classes = [\"positive\", \"negative\"]\n",
    "    def classify_text(text, possible_outputs):\n",
    "        framed_texts = [f\"{text} This text is {output}.\" for output in possible_outputs]\n",
    "        encoded_inputs = [tokenizer.encode(t, return_tensors=\"pt\").to(device) for t in framed_texts]\n",
    "        \n",
    "        logits_for_outputs = []\n",
    "\n",
    "        for encoded_input in encoded_inputs:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(encoded_input)\n",
    "                logits = outputs.logits\n",
    "            logits_for_outputs.append(logits[0, -1, :].squeeze().cpu().numpy())\n",
    "        \n",
    "        token_ids = [tokenizer.encode(output)[0] for output in possible_outputs]\n",
    "        class_logits = [logits[token_id] for logits, token_id in zip(logits_for_outputs, token_ids)]\n",
    "        return possible_outputs[class_logits.index(max(class_logits))]\n",
    "    dataset = load_dataset(\"glue\", \"sst2\")\n",
    "    train_data = dataset['train']\n",
    "\n",
    "    ground_truth = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for data in tqdm(train_data):\n",
    "        logits = data['label']\n",
    "        prediction = classify_text(data['sentence'], possible_classes)\n",
    "        if prediction == \"positive\":\n",
    "            pred_labels.append(1)\n",
    "        else:\n",
    "            pred_labels.append(0)\n",
    "        ground_truth.append(logits)\n",
    "    return accuracy_score(ground_truth, pred_labels)\n",
    "\n",
    "def evaluate_summarization(model, tokenizer, dataset=\"cnn_dailymail\", max_tokens=150, device=device):\n",
    "    def summarize(text, max_length=max_tokens):\n",
    "        prompt = f\"{text} TL;DR: \"\n",
    "\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            max_length = max_length + len(input_ids[0])\n",
    "            output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        summary = generated_text.split(\"TL;DR: \")[-1]\n",
    "        return summary\n",
    "\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    test_data = dataset['test']\n",
    "    rouge_1, rouge_2, rouge_l = [], [], []\n",
    "    for data in tqdm(test_data):\n",
    "        article = data['article']\n",
    "        summary = summarize(article)\n",
    "        ground_truth = data['highlights']\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(summary, ground_truth, avg=True)\n",
    "        score_1, score_2, score_l = scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f']\n",
    "        rouge_1.append(score_1)\n",
    "        rouge_2.append(score_2)\n",
    "        rouge_l.append(score_l)\n",
    "    return np.mean(rouge_1), np.mean(rouge_2), np.mean(rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/minhaoj2/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4ee5c406f84d53bc061f0b4baec92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11490 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate_summarization(model, tokenizer)\n",
      "Cell \u001b[0;32mIn[9], line 53\u001b[0m, in \u001b[0;36mevaluate_summarization\u001b[0;34m(model, tokenizer, dataset, max_tokens, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(test_data):\n\u001b[1;32m     52\u001b[0m     article \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 53\u001b[0m     summary \u001b[39m=\u001b[39m summarize(article)\n\u001b[1;32m     54\u001b[0m     ground_truth \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mhighlights\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     55\u001b[0m     rouge \u001b[39m=\u001b[39m Rouge()\n",
      "Cell \u001b[0;32mIn[9], line 39\u001b[0m, in \u001b[0;36mevaluate_summarization.<locals>.summarize\u001b[0;34m(text, max_length)\u001b[0m\n\u001b[1;32m     36\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m TL;DR: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     41\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     42\u001b[0m     max_length \u001b[39m=\u001b[39m max_length \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(input_ids[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "evaluate_summarization(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, max_length=150):\n",
    "    prompt = f\"{text} TL;DR: \"\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        max_length = max_length + len(input_ids[0])\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    summary = generated_text.split(\"TL;DR: \")[-1]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed\n",
      "The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted\n"
     ]
    }
   ],
   "source": [
    "text = \"The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed\"\n",
    "print(summarize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774030080"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-large\"  # You can choose other sizes as well\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).eval().cuda()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "sum(p.numel() for p in model.parameters())\n",
    "# def generate_summary(article):\n",
    "#     article = article + \"\\nTL;DR: \"\n",
    "#     tokens = tokenizer.tokenize(article)\n",
    "#     if len(tokens) > 924:\n",
    "#         tokens = tokens[-924:]\n",
    "#     input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokens)]).cuda()\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(\n",
    "#             input_ids, \n",
    "#             max_length=len(input_ids[0]) + 150, \n",
    "#             num_return_sequences=1, \n",
    "#             do_sample=True, \n",
    "#             top_k=2\n",
    "#         )\n",
    "#     generated_summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "#     sentences = generated_summary.split('.')\n",
    "#     if len(sentences) > 3:\n",
    "#         return '.'.join(sentences[:3]) + '.'\n",
    "#     else:\n",
    "#         return generated_summary\n",
    "\n",
    "# # Test the function\n",
    "# article = \"(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed\"\n",
    "# print(generate_summary(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
